{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "import open3d as o3d\n",
    "import torch # why is it located here?\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import pdb\n",
    "import cv2\n",
    "cv2.setNumThreads(0)\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from model import get as get_model\n",
    "from model.utils.configs import Config\n",
    "from model.utils.common_util import AverageMeter, intersectionAndUnion, find_free_port\n",
    "# from model import get as get_model\n",
    "# from dataset import get as get_dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed=0\n",
    "pl.seed_everything(seed) # , workers=True\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [int(x.strip()) for x in lines]\n",
    "    return lines\n",
    "\n",
    "#parser = my_args()\n",
    "args = Config()\n",
    "\n",
    "    # ------------\n",
    "    # randomness or seed\n",
    "    # ------------\n",
    "torch.backends.cudnn.benchmark = args.cudnn_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt best. args.load_model=[/home/fangj1/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/checkpoints/epoch=062--mIoU_val=0.6972--.ckpt]\n",
      "SymmetricTransitionDownBlock\n",
      "------------------- frozen ---------------------\n",
      "class_embedding\n",
      "positional_embedding\n",
      "proj\n",
      "conv1.weight\n",
      "ln_pre.weight\n",
      "ln_pre.bias\n",
      "transformer.resblocks.0.attn.in_proj_weight\n",
      "transformer.resblocks.0.attn.in_proj_bias\n",
      "transformer.resblocks.0.attn.out_proj.weight\n",
      "transformer.resblocks.0.attn.out_proj.bias\n",
      "transformer.resblocks.0.ln_1.weight\n",
      "transformer.resblocks.0.ln_1.bias\n",
      "transformer.resblocks.0.mlp.c_fc.weight\n",
      "transformer.resblocks.0.mlp.c_fc.bias\n",
      "transformer.resblocks.0.mlp.c_proj.weight\n",
      "transformer.resblocks.0.mlp.c_proj.bias\n",
      "transformer.resblocks.0.ln_2.weight\n",
      "transformer.resblocks.0.ln_2.bias\n",
      "transformer.resblocks.1.attn.in_proj_weight\n",
      "transformer.resblocks.1.attn.in_proj_bias\n",
      "transformer.resblocks.1.attn.out_proj.weight\n",
      "transformer.resblocks.1.attn.out_proj.bias\n",
      "transformer.resblocks.1.ln_1.weight\n",
      "transformer.resblocks.1.ln_1.bias\n",
      "transformer.resblocks.1.mlp.c_fc.weight\n",
      "transformer.resblocks.1.mlp.c_fc.bias\n",
      "transformer.resblocks.1.mlp.c_proj.weight\n",
      "transformer.resblocks.1.mlp.c_proj.bias\n",
      "transformer.resblocks.1.ln_2.weight\n",
      "transformer.resblocks.1.ln_2.bias\n",
      "transformer.resblocks.2.attn.in_proj_weight\n",
      "transformer.resblocks.2.attn.in_proj_bias\n",
      "transformer.resblocks.2.attn.out_proj.weight\n",
      "transformer.resblocks.2.attn.out_proj.bias\n",
      "transformer.resblocks.2.ln_1.weight\n",
      "transformer.resblocks.2.ln_1.bias\n",
      "transformer.resblocks.2.mlp.c_fc.weight\n",
      "transformer.resblocks.2.mlp.c_fc.bias\n",
      "transformer.resblocks.2.mlp.c_proj.weight\n",
      "transformer.resblocks.2.mlp.c_proj.bias\n",
      "transformer.resblocks.2.ln_2.weight\n",
      "transformer.resblocks.2.ln_2.bias\n",
      "transformer.resblocks.3.attn.in_proj_weight\n",
      "transformer.resblocks.3.attn.in_proj_bias\n",
      "transformer.resblocks.3.attn.out_proj.weight\n",
      "transformer.resblocks.3.attn.out_proj.bias\n",
      "transformer.resblocks.3.ln_1.weight\n",
      "transformer.resblocks.3.ln_1.bias\n",
      "transformer.resblocks.3.mlp.c_fc.weight\n",
      "transformer.resblocks.3.mlp.c_fc.bias\n",
      "transformer.resblocks.3.mlp.c_proj.weight\n",
      "transformer.resblocks.3.mlp.c_proj.bias\n",
      "transformer.resblocks.3.ln_2.weight\n",
      "transformer.resblocks.3.ln_2.bias\n",
      "transformer.resblocks.4.attn.in_proj_weight\n",
      "transformer.resblocks.4.attn.in_proj_bias\n",
      "transformer.resblocks.4.attn.out_proj.weight\n",
      "transformer.resblocks.4.attn.out_proj.bias\n",
      "transformer.resblocks.4.ln_1.weight\n",
      "transformer.resblocks.4.ln_1.bias\n",
      "transformer.resblocks.4.mlp.c_fc.weight\n",
      "transformer.resblocks.4.mlp.c_fc.bias\n",
      "transformer.resblocks.4.mlp.c_proj.weight\n",
      "transformer.resblocks.4.mlp.c_proj.bias\n",
      "transformer.resblocks.4.ln_2.weight\n",
      "transformer.resblocks.4.ln_2.bias\n",
      "transformer.resblocks.5.attn.in_proj_weight\n",
      "transformer.resblocks.5.attn.in_proj_bias\n",
      "transformer.resblocks.5.attn.out_proj.weight\n",
      "transformer.resblocks.5.attn.out_proj.bias\n",
      "transformer.resblocks.5.ln_1.weight\n",
      "transformer.resblocks.5.ln_1.bias\n",
      "transformer.resblocks.5.mlp.c_fc.weight\n",
      "transformer.resblocks.5.mlp.c_fc.bias\n",
      "transformer.resblocks.5.mlp.c_proj.weight\n",
      "transformer.resblocks.5.mlp.c_proj.bias\n",
      "transformer.resblocks.5.ln_2.weight\n",
      "transformer.resblocks.5.ln_2.bias\n",
      "transformer.resblocks.6.attn.in_proj_weight\n",
      "transformer.resblocks.6.attn.in_proj_bias\n",
      "transformer.resblocks.6.attn.out_proj.weight\n",
      "transformer.resblocks.6.attn.out_proj.bias\n",
      "transformer.resblocks.6.ln_1.weight\n",
      "transformer.resblocks.6.ln_1.bias\n",
      "transformer.resblocks.6.mlp.c_fc.weight\n",
      "transformer.resblocks.6.mlp.c_fc.bias\n",
      "transformer.resblocks.6.mlp.c_proj.weight\n",
      "transformer.resblocks.6.mlp.c_proj.bias\n",
      "transformer.resblocks.6.ln_2.weight\n",
      "transformer.resblocks.6.ln_2.bias\n",
      "transformer.resblocks.7.attn.in_proj_weight\n",
      "transformer.resblocks.7.attn.in_proj_bias\n",
      "transformer.resblocks.7.attn.out_proj.weight\n",
      "transformer.resblocks.7.attn.out_proj.bias\n",
      "transformer.resblocks.7.ln_1.weight\n",
      "transformer.resblocks.7.ln_1.bias\n",
      "transformer.resblocks.7.mlp.c_fc.weight\n",
      "transformer.resblocks.7.mlp.c_fc.bias\n",
      "transformer.resblocks.7.mlp.c_proj.weight\n",
      "transformer.resblocks.7.mlp.c_proj.bias\n",
      "transformer.resblocks.7.ln_2.weight\n",
      "transformer.resblocks.7.ln_2.bias\n",
      "transformer.resblocks.8.attn.in_proj_weight\n",
      "transformer.resblocks.8.attn.in_proj_bias\n",
      "transformer.resblocks.8.attn.out_proj.weight\n",
      "transformer.resblocks.8.attn.out_proj.bias\n",
      "transformer.resblocks.8.ln_1.weight\n",
      "transformer.resblocks.8.ln_1.bias\n",
      "transformer.resblocks.8.mlp.c_fc.weight\n",
      "transformer.resblocks.8.mlp.c_fc.bias\n",
      "transformer.resblocks.8.mlp.c_proj.weight\n",
      "transformer.resblocks.8.mlp.c_proj.bias\n",
      "transformer.resblocks.8.ln_2.weight\n",
      "transformer.resblocks.8.ln_2.bias\n",
      "transformer.resblocks.9.attn.in_proj_weight\n",
      "transformer.resblocks.9.attn.in_proj_bias\n",
      "transformer.resblocks.9.attn.out_proj.weight\n",
      "transformer.resblocks.9.attn.out_proj.bias\n",
      "transformer.resblocks.9.ln_1.weight\n",
      "transformer.resblocks.9.ln_1.bias\n",
      "transformer.resblocks.9.mlp.c_fc.weight\n",
      "transformer.resblocks.9.mlp.c_fc.bias\n",
      "transformer.resblocks.9.mlp.c_proj.weight\n",
      "transformer.resblocks.9.mlp.c_proj.bias\n",
      "transformer.resblocks.9.ln_2.weight\n",
      "transformer.resblocks.9.ln_2.bias\n",
      "transformer.resblocks.10.attn.in_proj_weight\n",
      "transformer.resblocks.10.attn.in_proj_bias\n",
      "transformer.resblocks.10.attn.out_proj.weight\n",
      "transformer.resblocks.10.attn.out_proj.bias\n",
      "transformer.resblocks.10.ln_1.weight\n",
      "transformer.resblocks.10.ln_1.bias\n",
      "transformer.resblocks.10.mlp.c_fc.weight\n",
      "transformer.resblocks.10.mlp.c_fc.bias\n",
      "transformer.resblocks.10.mlp.c_proj.weight\n",
      "transformer.resblocks.10.mlp.c_proj.bias\n",
      "transformer.resblocks.10.ln_2.weight\n",
      "transformer.resblocks.10.ln_2.bias\n",
      "transformer.resblocks.11.attn.in_proj_weight\n",
      "transformer.resblocks.11.attn.in_proj_bias\n",
      "transformer.resblocks.11.attn.out_proj.weight\n",
      "transformer.resblocks.11.attn.out_proj.bias\n",
      "transformer.resblocks.11.ln_1.weight\n",
      "transformer.resblocks.11.ln_1.bias\n",
      "transformer.resblocks.11.mlp.c_fc.weight\n",
      "transformer.resblocks.11.mlp.c_fc.bias\n",
      "transformer.resblocks.11.mlp.c_proj.weight\n",
      "transformer.resblocks.11.mlp.c_proj.bias\n",
      "transformer.resblocks.11.ln_2.weight\n",
      "transformer.resblocks.11.ln_2.bias\n",
      "ln_post.weight\n",
      "ln_post.bias\n",
      "------------------- frozen ---------------------\n"
     ]
    }
   ],
   "source": [
    "# load check point for the model\n",
    "from importlib import import_module\n",
    "args.load_model =\"/home/fangj1/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/checkpoints/epoch=062--mIoU_val=0.6972--.ckpt\"\n",
    "args.on_train = False\n",
    "print('ckpt best. args.load_model=[{}]'.format(args.load_model))\n",
    "assert args.load_model is not None, 'why did you come?'\n",
    "print(args.transdown)\n",
    "model = get_model(args.model).load_from_checkpoint(\n",
    "    os.path.join(args.MYCHECKPOINT, args.load_model), \n",
    "    args=args).to(device) # args.strict_load\n",
    "\n",
    "model.eval()\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process arkit data\n",
    "in original model need (n, 3), (n, c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data loader\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "class ArkitDataLoader(Dataset):\n",
    "    def __init__(self, scene_path , query_path = None, mask_path = None):\n",
    "        \"\"\"\n",
    "        初始化点云数据集\n",
    "        :param ply_files: PLY文件的列表\n",
    "        \"\"\"\n",
    "        self.scene_path = scene_path\n",
    "        self.query_path = query_path\n",
    "        self.mask_path = mask_path\n",
    "        self.ply_list = []  # 先初始化列表\n",
    "        self.query_list = []\n",
    "        self.mask_list = []\n",
    "        self.scene_id = None\n",
    "        \n",
    "        # 然后加载数据\n",
    "        self.load_ply_list()\n",
    "        self.load_query_list()\n",
    "        self.load_mask_list()\n",
    "        \n",
    "    def load_ply_list(self):\n",
    "        \"\"\"\n",
    "        加载PLY文件列表\n",
    "        \"\"\"\n",
    "        # 读取PLY文件列表\n",
    "        self.ply_list = sorted(glob.glob(os.path.join(self.scene_path, '4*/*.ply')))\n",
    "        #print(\"Loaded PLY files:\", self.ply_list)\n",
    "    def load_query_list(self):\n",
    "        if self.query_path is not None:\n",
    "            # 加载CSV文件\n",
    "            df = pd.read_csv(self.query_path)\n",
    "\n",
    "            # 按照某个列的值进行排序\n",
    "            df_sorted = df.sort_values(by='video_id')\n",
    "\n",
    "            # 获取排序后的另一个列的值\n",
    "            self.query_list = df_sorted['query'].values\n",
    "            # # 输出获取的值\n",
    "            # print(self.query_list)\n",
    "            \n",
    "    def load_mask_list(self):\n",
    "        if self.mask_path is not None:\n",
    "            self.mask_list = sorted(glob.glob(os.path.join(self.mask_path, '*.txt')))\n",
    "            #print(self.mask_list)\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        数据集中的样本数\n",
    "        \"\"\"\n",
    "        return len(self.ply_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        读取单个点云文件，并返回其数据\n",
    "        :param idx: 索引\n",
    "        \"\"\"\n",
    "        self.scene_id = self.mask_list[idx].split('/')[-1].split('_')[0]\n",
    "        #print(self.ply_list[idx].split('/')[-1].split('_')[0])\n",
    "        # 加载点云文件\n",
    "        pcd = o3d.io.read_point_cloud(self.ply_list[idx])\n",
    "\n",
    "        # 获取坐标\n",
    "        coordinates = np.asarray(pcd.points, dtype=np.float32)\n",
    "\n",
    "        # 获取特征，这里假设使用颜色作为特征\n",
    "        if pcd.colors:\n",
    "            features = np.asarray(pcd.colors, dtype=np.float32)  # RGB颜色\n",
    "        else:\n",
    "            features = np.zeros((coordinates.shape[0], 3), dtype=np.float32)  # 如果没有颜色，使用零填充\n",
    "\n",
    "        \n",
    "        #mask\n",
    "        if self.mask_path is not None:\n",
    "            mask = read_txt(self.mask_list[idx])\n",
    "        # 将数据转换为torch tensors\n",
    "        mask = torch.tensor(mask).unsqueeze(1)\n",
    "        coordinates = torch.from_numpy(coordinates)\n",
    "        features = torch.from_numpy(features)\n",
    "\n",
    "        #return {'coord': coordinates, 'feat': features, 'prompt': self.query_list[idx], 'target': mask}\n",
    "        return coordinates, features, self.query_list[idx], mask\n",
    "# def collate_fn(batch):\n",
    "#     max_points = max([sample['coord'].shape[0] for sample in batch])\n",
    "#     feature_dim = batch[0]['feat'].shape[1]\n",
    "\n",
    "#     batch_coordinates = torch.zeros(len(batch), max_points, 3, device=device)\n",
    "#     batch_features = torch.zeros(len(batch), max_points, feature_dim, device=device)\n",
    "#     batch_prompts = []\n",
    "#     batch_masks = torch.zeros(len(batch), max_points, 1,device=device)\n",
    "\n",
    "#     for i, sample in enumerate(batch):\n",
    "#         num_points = sample['coord'].shape[0]\n",
    "#         batch_coordinates[i, :num_points] = sample['coord'].to(device)\n",
    "#         batch_features[i, :num_points] = sample['feat'].to(device)\n",
    "#         batch_prompts.append(sample['prompt'])\n",
    "#         batch_masks[i, :num_points] = sample['target'].to(device)\n",
    "    \n",
    "#     offset, count = [], 0\n",
    "#     for item in max_points: # len of pc\n",
    "#         count += item.shape[0]\n",
    "#         offset.append(count)   \n",
    "#     return {\n",
    "#         'coord': batch_coordinates,\n",
    "#         'feat': batch_features,\n",
    "#         'offset': torch.IntTensor(batch_offsets),\n",
    "#         'prompt': batch_prompts,\n",
    "#         'target': batch_masks\n",
    "#     }\n",
    "def TrainValCollateFn(batch):\n",
    "    coord, feat, prompt, mask = list(zip(*batch))\n",
    "    offset, count = [], 0\n",
    "    for item in coord: # len of pc\n",
    "        count += item.shape[0]\n",
    "        offset.append(count)\n",
    "    \n",
    "    print(\"Coordinates type:\", type(coord[0]))  # Check the type of the first coordinate set\n",
    "    print(\"Features type:\", type(feat[0]))      # Check the type of the first features set\n",
    "    print(\"Mask type:\", type(mask[0]))          # Check the type of the first mask\n",
    "    print(\"Offset type:\", type(offset[0]))      # Check the type of the first offset\n",
    "    \n",
    "    data_dict = \\\n",
    "        {\n",
    "            'coord': torch.cat(coord).to(device),\n",
    "            'feat': torch.cat(feat).to(device),\n",
    "            'target': torch.cat(mask).to(device),\n",
    "            'prompt': list(prompt),\n",
    "            'offset': torch.IntTensor(offset).to(device),\n",
    "        }\n",
    "    return data_dict\n",
    "dataset = ArkitDataLoader(args.arkit_train_root,args.development_query_root,args.development_mask_root)\n",
    "#data_loader = DataLoader(dataset, batch_size= args.train_batch, collate_fn=TrainValCollateFn)\n",
    "data_loader = DataLoader(dataset, batch_size= 1, collate_fn=TrainValCollateFn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test input with Arkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates type: <class 'torch.Tensor'>\n",
      "Features type: <class 'torch.Tensor'>\n",
      "Mask type: <class 'torch.Tensor'>\n",
      "Offset type: <class 'int'>\n",
      "coord.shape torch.Size([1714989, 3])\n",
      "feat.shape torch.Size([1714989, 3])\n",
      "target.shape torch.Size([1714989, 1])\n",
      "offset.shape torch.Size([1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.64 GiB (GPU 0; 10.75 GiB total capacity; 8.82 GiB already allocated; 332.81 MiB free; 9.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 获取第一个元素\u001b[39;00m\n\u001b[1;32m      4\u001b[0m first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_iterator)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/model/net_epcl.py:103\u001b[0m, in \u001b[0;36mnet_epcl.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, target\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, offset\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 103\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minds_recons\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data_dict:\n\u001b[1;32m    105\u001b[0m         inds_reverse \u001b[38;5;241m=\u001b[39m data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minds_recons\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/model/network/epcl.py:265\u001b[0m, in \u001b[0;36mEPCLSegNet.forward\u001b[0;34m(self, pxo)\u001b[0m\n\u001b[1;32m    263\u001b[0m p0, x0, o0 \u001b[38;5;241m=\u001b[39m pxo  \u001b[38;5;66;03m# (n, 3), (n, c), (b)->xyz,feature,batch_size\u001b[39;00m\n\u001b[1;32m    264\u001b[0m x0 \u001b[38;5;241m=\u001b[39m p0 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((p0, x0), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 265\u001b[0m p1, x1, o1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 1 6 -> 32\u001b[39;00m\n\u001b[1;32m    266\u001b[0m p2, x2, o2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc2([p1, x1, o1]) \u001b[38;5;66;03m# 4 32 -> 64\u001b[39;00m\n\u001b[1;32m    267\u001b[0m p3, x3, o3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc3([p2, x2, o2]) \u001b[38;5;66;03m# 16 64 -> 128\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/model/network/epcl.py:159\u001b[0m, in \u001b[0;36mPointMixerBlock.forward\u001b[0;34m(self, pxo)\u001b[0m\n\u001b[1;32m    157\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    158\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x)))\n\u001b[0;32m--> 159\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    160\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear3(x))\n\u001b[1;32m    161\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m identity\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/EPCL/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/model/network/epcl.py:127\u001b[0m, in \u001b[0;36mPointMixerIntraSetLayer.forward\u001b[0;34m(self, pxo)\u001b[0m\n\u001b[1;32m    124\u001b[0m n, nsample, out_planes \u001b[38;5;241m=\u001b[39m x_v\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    125\u001b[0m x_knn \u001b[38;5;241m=\u001b[39m (x_v \u001b[38;5;241m+\u001b[39m p_embed)\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    126\u001b[0m     n, nsample, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_planes, out_planes\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_planes)\n\u001b[0;32m--> 127\u001b[0m x_knn \u001b[38;5;241m=\u001b[39m (\u001b[43mx_knn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    128\u001b[0m x_knn \u001b[38;5;241m=\u001b[39m x_knn\u001b[38;5;241m.\u001b[39mreshape(n, nsample, out_planes)\n\u001b[1;32m    130\u001b[0m x \u001b[38;5;241m=\u001b[39m x_knn\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.64 GiB (GPU 0; 10.75 GiB total capacity; 8.82 GiB already allocated; 332.81 MiB free; 9.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "data_iterator = iter(data_loader)\n",
    "\n",
    "# 获取第一个元素\n",
    "first_batch = next(data_iterator)\n",
    "model(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "cls = nn.Sequential(\n",
    "    nn.Linear(32, 128), \n",
    "    nn.BatchNorm1d(128), \n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 512), \n",
    "    nn.BatchNorm1d(512), \n",
    "    nn.ReLU(inplace=True))\n",
    "cls = cls.to(device)\n",
    "\n",
    "\n",
    "#model.model.cls =cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(num_ftrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPCL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
