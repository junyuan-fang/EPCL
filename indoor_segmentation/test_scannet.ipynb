{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plyfile in /users/fangjuny/.local/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib64/python3.9/site-packages (from plyfile) (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: easydict in /users/fangjuny/.local/lib/python3.9/site-packages (1.13)\n"
     ]
    }
   ],
   "source": [
    "! pip install plyfile\n",
    "! pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clip\n",
      "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6988 sha256=235ef31045dd19149c95a1ab3bc90fc5e9aa3a67c09d9b050bef1b0e4c2ac15b\n",
      "  Stored in directory: /users/fangjuny/.cache/pip/wheels/ab/a5/e8/c9fa20742edbccf2702dae8ee62053e6c460e961d45967b49c\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-0.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCLIPTextEncoder\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CLIPTextEncoder(nn.Module):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        emb_dim = kwargs.get(\"embed_dim\")#768\n",
    "        self.token_embedding, self.clip_txt, self.transformer, self.ln_final, self.text_projection = self.clip_txt_transformer()\n",
    "\n",
    "    def clip_txt_transformer(self, freeze=True):\n",
    "    \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        clip_model_name = 'ViT-B/32'\n",
    "        clip_model = clip.load(clip_model_name, device=device)[0]\n",
    "\n",
    "        clip_txt = clip_model.transformer\n",
    "        clip_txt.float()\n",
    "        if freeze:\n",
    "            print(\"------------------- Txt frozen ---------------------\")\n",
    "            clip_txt.eval()\n",
    "            for k, v in clip_txt.named_parameters():\n",
    "                v.requires_grad = False\n",
    "                print(k)\n",
    "            print(\"------------------- Txtfrozen ---------------------\")\n",
    "            #transformer's input output are the same(sequence length, 1, feature dimension), \n",
    "            #class_embedding\n",
    "        return clip_txt.token_embedding, clip_txt.positional_embedding, clip_txt.txt.transformer, clip_txt.ln_final, clip_txt.text_projection\n",
    "\n",
    "    def encode_text(self, text, apply_projection=True):\n",
    "        x = self.token_embedding(text).type(self.dtype)\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        if apply_projection:\n",
    "            # 只在需要时应用投影层\n",
    "            x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        else:\n",
    "            # 返回没有应用投影层的特征\n",
    "            x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, text, apply_text_projection=True):\n",
    "        text_features = self.encode_text(text, apply_projection=apply_text_projection)\n",
    "        return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: SharedArray in /users/fangjuny/.local/lib/python3.9/site-packages (3.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib64/python3.9/site-packages (from SharedArray) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install SharedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "import open3d as o3d\n",
    "import torch # why is it located here?\n",
    "import numpy as np\n",
    "from plyfile import PlyData, PlyElement\n",
    "import pdb\n",
    "import cv2\n",
    "cv2.setNumThreads(0)\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from model import get as get_model\n",
    "from configs.configs import Config_train\n",
    "# from model import get as get_model\n",
    "from dataset import get as get_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed=0\n",
    "pl.seed_everything(seed) # , workers=True\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
    "\n",
    "def read_txt(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [int(x.strip()) for x in lines]\n",
    "    return lines\n",
    "\n",
    "#parser = my_args()\n",
    "args = Config_train()\n",
    "\n",
    "    # ------------\n",
    "    # randomness or seed\n",
    "    # ------------\n",
    "torch.backends.cudnn.benchmark = args.cudnn_benchmark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_path = \"/scratch/project_2002051/junyuan/cvpr24-challenge/data/ScanNet200/train\"\n",
    "data_ROOT = \"/scratch/project_2002051/junyuan/cvpr24-challenge/data/ScanNet200\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.utils import transform as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element vertex 104270\n",
      "property float x\n",
      "property float y\n",
      "property float z\n",
      "property uchar red\n",
      "property uchar green\n",
      "property uchar blue\n",
      "property uint label\n",
      "property uint instance_id\n",
      "(104270, 3)\n"
     ]
    }
   ],
   "source": [
    "with open('/scratch/project_2002051/junyuan/cvpr24-challenge/data/ScanNet200/train/scene0691_00.ply', 'rb') as f:\n",
    "    plydata = PlyData.read(f)\n",
    "    print(plydata['vertex'])\n",
    "    vertex = plydata['vertex']\n",
    "    \n",
    "    #coordinates\n",
    "    coord = np.vstack([vertex['x'], vertex['y'], vertex['z']]).T\n",
    "    print(coord.shape)\n",
    "    # Extract colors\n",
    "    feat = np.vstack([vertex['red'], vertex['green'], vertex['blue']]).T\n",
    "        \n",
    "    # Extract labels\n",
    "    label = vertex['label']\n",
    "\n",
    "    # Extract instant_id\n",
    "    instant_id = vertex['instance_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.utils.data_util import data_prepare_v101 as data_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this dataloade, the data augmentation for s3dis dataset can be directly used in scannet dataset\n",
    "# the original scale of RGB in scannet dataset is [0, 255]\n",
    "# after augmentation and pre-processing, the scale of RGB will be converted into [0, 1]\n",
    "\n",
    "# the scale of coordinate keeps the same and only has [coord_min] shift  \n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self, scene_path, query_path = None, mask_path = None, mode = 'train'):\n",
    "        \"\"\"\n",
    "        初始化点云数据集\n",
    "        :param ply_files: PLY文件的列表\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.scene_path = \"/scratch/project_2002051/junyuan/cvpr24-challenge/data/ScanNet200/train\"\n",
    "        self.query_path = query_path\n",
    "        self.mask_path = mask_path\n",
    "        \n",
    "        self.voxel_size = 0.04\n",
    "        self.test_area = 5\n",
    "        self.classes = None\n",
    "\n",
    "        self.shuffle_index = True\n",
    "        self.voxel_max = 8000000\n",
    "        \n",
    "        self.transform = t.Compose(\n",
    "                [\n",
    "                    t.RandomScale([0.9, 1.1]), \n",
    "                    t.ChromaticAutoContrast(),\n",
    "                    t.ChromaticTranslation(), \n",
    "                    t.ChromaticJitter(), \n",
    "                    t.HueSaturationTranslation()\n",
    "                ])\n",
    "        \n",
    "        self.ply_list = []  # 先初始化列表\n",
    "        self.query_list = []\n",
    "        self.mask_list = []\n",
    "        self.scene_id = None\n",
    "        \n",
    "        # 然后加载数据\n",
    "        self.load_ply_list()\n",
    "        #self.load_query_list()\n",
    "        #self.load_mask_list()\n",
    "    \n",
    "    def load_ply_list(self):\n",
    "        \"\"\"\n",
    "        加载PLY文件列表\n",
    "        \"\"\"\n",
    "        # 读取PLY文件列表\n",
    "        self.ply_list = sorted(glob.glob(os.path.join(self.scene_path, '*.ply')))\n",
    "        #self.data_idx = np.arange(len(self.ply_list))\n",
    "        \n",
    "        #print(\"Loaded PLY files:\", self.ply_list)\n",
    "    def load_query_list(self):\n",
    "        if self.query_path is not None:\n",
    "            # 加载CSV文件\n",
    "            df = pd.read_csv(self.query_path)\n",
    "\n",
    "            # 按照某个列的值进行排序\n",
    "            df_sorted = df.sort_values(by='video_id')\n",
    "\n",
    "            # 获取排序后的另一个列的值\n",
    "            self.query_list = df_sorted['query'].values\n",
    "            # # 输出获取的值\n",
    "            # print(self.query_list)\n",
    "            \n",
    "    def load_mask_list(self):\n",
    "        if self.mask_path is not None:\n",
    "            self.mask_list = sorted(glob.glob(os.path.join(self.mask_path, '*.txt')))\n",
    "            #print(self.mask_list)\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        数据集中的样本数\n",
    "        \"\"\"\n",
    "        return len(self.ply_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        plydata = PlyData.read(self.ply_list[idx])\n",
    "        vertex_data = plydata['vertex']\n",
    "\n",
    "        #coordinates\n",
    "        coord = np.vstack([vertex['x'], vertex['y'], vertex['z']]).T\n",
    "        # Extract colors\n",
    "        feat = np.vstack([vertex['red'], vertex['green'], vertex['blue']]).T\n",
    "        # Extract labels\n",
    "        label = vertex['label']\n",
    "        # Extract instant_id\n",
    "        instance_id = vertex['instance_id']\n",
    "        # Optionally, convert labels to a numpy array for easier manipulation\n",
    "        labels_array = label.astype(np.int64)\n",
    "        coord, feat, label = data_prepare(\n",
    "            coord, feat, labels_array, \n",
    "            self.mode, self.voxel_size, self.voxel_max, \n",
    "            self.transform, self.shuffle_index)\n",
    "        \n",
    "        return coord, feat, label, instance_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetA(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.file_list = sorted(glob.glob(os.path.join(self.path, '*.ply')))\n",
    "        self.load_files()\n",
    "\n",
    "    def load_files(self):\n",
    "        self.data = []\n",
    "        for file in self.file_list:\n",
    "            plydata = PlyData.read(file)\n",
    "            vertex_data = plydata['vertex']\n",
    "            coord = np.vstack([vertex_data['x'], vertex_data['y'], vertex_data['z']]).T\n",
    "            feat = np.vstack([vertex_data['red'], vertex_data['green'], vertex_data['blue']]).T\n",
    "            label = vertex_data['label'].astype(np.int64)\n",
    "            instance_id = vertex_data['instance_id']\n",
    "            self.data.append((coord, feat, label, instance_id))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coord, feat, label, instance_id = self.data[idx]\n",
    "        if self.transform:\n",
    "            coord, feat, label = self.transform(coord, feat, label)\n",
    "        return coord, feat, label, instance_id\n",
    "\n",
    "\n",
    "class DatasetB(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.file_list = sorted(glob.glob(os.path.join(self.path, '*.ply')))\n",
    "        self.load_files()\n",
    "\n",
    "    def load_files(self):\n",
    "        self.data = []\n",
    "        for file in self.file_list:\n",
    "            plydata = PlyData.read(file)\n",
    "            vertex_data = plydata['vertex']\n",
    "            coord = np.vstack([vertex_data['x'], vertex_data['y'], vertex_data['z']]).T\n",
    "            feat = np.vstack([vertex_data['red'], vertex_data['green'], vertex_data['blue']]).T\n",
    "            label = vertex_data['label'].astype(np.int64)\n",
    "            instance_id = vertex_data['instance_id']\n",
    "            self.data.append((coord, feat, label, instance_id))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coord, feat, label, instance_id = self.data[idx]\n",
    "        if self.transform:\n",
    "            coord, feat, label = self.transform(coord, feat, label)\n",
    "        return coord, feat, label, instance_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scannetloader = Loader(scene_path=None)\n",
    "\n",
    "data_iterator = iter(scannetloader)\n",
    "\n",
    "# 获取第一个元素\n",
    "first_batch = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 4,  ..., 3, 3, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[2]\n",
    "\n",
    "#torch.Size([104270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainValCollateFn(batch):\n",
    "    coord, feat, label, _ = list(zip(*batch))\n",
    "    offset, count = [], 0\n",
    "    for item in coord: # len of pc\n",
    "        count += item.shape[0]\n",
    "        offset.append(count)\n",
    "    data_dict = \\\n",
    "        {\n",
    "            'coord': torch.cat(coord),\n",
    "            'feat': torch.cat(feat),\n",
    "            'target': torch.cat(label),\n",
    "            'offset': torch.IntTensor(offset),\n",
    "        }\n",
    "    return data_dict\n",
    "\n",
    "train_loader_kwargs = \\\n",
    "        {\n",
    "            \"batch_size\": 4,\n",
    "            \"num_workers\": 2,\n",
    "            \"collate_fn\": TrainValCollateFn,\n",
    "            \"pin_memory\": True,\n",
    "            \"drop_last\": False,\n",
    "            \"shuffle\": True,\n",
    "        }\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        scannetloader, **train_loader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BatchSampler' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BatchSampler' object is not callable"
     ]
    }
   ],
   "source": [
    "train_loader.batch_sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt best. args.load_model=[/home/fangj1/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/checkpoints/epoch=062--mIoU_val=0.6972--.ckpt]\n",
      "SymmetricTransitionDownBlock\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/fangj1/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/checkpoints/epoch=062--mIoU_val=0.6972--.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m args\u001b[38;5;241m.\u001b[39mload_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhy did you come?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(args\u001b[38;5;241m.\u001b[39mtransdown)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMYCHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# args.strict_load\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pytorch_lightning/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     67\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/lightning_fabric/utilities/cloud_io.py:55\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     52\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/fsspec/spec.py:1307\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1307\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/fsspec/implementations/local.py:180\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/fsspec/implementations/local.py:302\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/fsspec/implementations/local.py:307\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    309\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/fangj1/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/checkpoints/epoch=062--mIoU_val=0.6972--.ckpt'"
     ]
    }
   ],
   "source": [
    "# load check point for the model\n",
    "from importlib import import_module\n",
    "args.load_model =\"/home/fangj1/Code/Vision-Language-on-3D-Scene-Understanding/EPCL/indoor_segmentation/checkpoints/epoch=062--mIoU_val=0.6972--.ckpt\"\n",
    "args.on_train = False\n",
    "print('ckpt best. args.load_model=[{}]'.format(args.load_model))\n",
    "assert args.load_model is not None, 'why did you come?'\n",
    "print(args.transdown)\n",
    "model = get_model(args.model).load_from_checkpoint(\n",
    "    os.path.join(args.MYCHECKPOINT, args.load_model), \n",
    "    args=args).to(device) # args.strict_load\n",
    "\n",
    "model.eval()\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process arkit data\n",
    "in original model need (n, 3), (n, c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data loader\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "class ArkitDataLoader(Dataset):\n",
    "    def __init__(self, scene_path , query_path = None, mask_path = None):\n",
    "        \"\"\"\n",
    "        初始化点云数据集\n",
    "        :param ply_files: PLY文件的列表\n",
    "        \"\"\"\n",
    "        self.scene_path = scene_path\n",
    "        self.query_path = query_path\n",
    "        self.mask_path = mask_path\n",
    "        self.ply_list = []  # 先初始化列表\n",
    "        self.query_list = []\n",
    "        self.mask_list = []\n",
    "        self.scene_id = None\n",
    "        \n",
    "        # 然后加载数据\n",
    "        self.load_ply_list()\n",
    "        self.load_query_list()\n",
    "        self.load_mask_list()\n",
    "        \n",
    "    def load_ply_list(self):\n",
    "        \"\"\"\n",
    "        加载PLY文件列表\n",
    "        \"\"\"\n",
    "        # 读取PLY文件列表\n",
    "        self.ply_list = sorted(glob.glob(os.path.join(self.scene_path, '4*/*.ply')))\n",
    "        #print(\"Loaded PLY files:\", self.ply_list)\n",
    "    def load_query_list(self):\n",
    "        if self.query_path is not None:\n",
    "            # 加载CSV文件\n",
    "            df = pd.read_csv(self.query_path)\n",
    "\n",
    "            # 按照某个列的值进行排序\n",
    "            df_sorted = df.sort_values(by='video_id')\n",
    "\n",
    "            # 获取排序后的另一个列的值\n",
    "            self.query_list = df_sorted['query'].values\n",
    "            # # 输出获取的值\n",
    "            # print(self.query_list)\n",
    "            \n",
    "    def load_mask_list(self):\n",
    "        if self.mask_path is not None:\n",
    "            self.mask_list = sorted(glob.glob(os.path.join(self.mask_path, '*.txt')))\n",
    "            #print(self.mask_list)\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        数据集中的样本数\n",
    "        \"\"\"\n",
    "        return len(self.ply_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        读取单个点云文件，并返回其数据\n",
    "        :param idx: 索引\n",
    "        \"\"\"\n",
    "        self.scene_id = self.mask_list[idx].split('/')[-1].split('_')[0]\n",
    "        #print(self.ply_list[idx].split('/')[-1].split('_')[0])\n",
    "        # 加载点云文件\n",
    "        pcd = o3d.io.read_point_cloud(self.ply_list[idx])\n",
    "\n",
    "        # 获取坐标\n",
    "        coordinates = np.asarray(pcd.points, dtype=np.float32)\n",
    "\n",
    "        # 获取特征，这里假设使用颜色作为特征\n",
    "        if pcd.colors:\n",
    "            features = np.asarray(pcd.colors, dtype=np.float32)  # RGB颜色\n",
    "        else:\n",
    "            features = np.zeros((coordinates.shape[0], 3), dtype=np.float32)  # 如果没有颜色，使用零填充\n",
    "\n",
    "        \n",
    "        #mask\n",
    "        if self.mask_path is not None:\n",
    "            mask = read_txt(self.mask_list[idx])\n",
    "        # 将数据转换为torch tensors\n",
    "        mask = torch.tensor(mask).unsqueeze(1)\n",
    "        coordinates = torch.from_numpy(coordinates)\n",
    "        features = torch.from_numpy(features)\n",
    "\n",
    "        #return {'coord': coordinates, 'feat': features, 'prompt': self.query_list[idx], 'target': mask}\n",
    "        return coordinates, features, self.query_list[idx], mask\n",
    "# def collate_fn(batch):\n",
    "#     max_points = max([sample['coord'].shape[0] for sample in batch])\n",
    "#     feature_dim = batch[0]['feat'].shape[1]\n",
    "\n",
    "#     batch_coordinates = torch.zeros(len(batch), max_points, 3, device=device)\n",
    "#     batch_features = torch.zeros(len(batch), max_points, feature_dim, device=device)\n",
    "#     batch_prompts = []\n",
    "#     batch_masks = torch.zeros(len(batch), max_points, 1,device=device)\n",
    "\n",
    "#     for i, sample in enumerate(batch):\n",
    "#         num_points = sample['coord'].shape[0]\n",
    "#         batch_coordinates[i, :num_points] = sample['coord'].to(device)\n",
    "#         batch_features[i, :num_points] = sample['feat'].to(device)\n",
    "#         batch_prompts.append(sample['prompt'])\n",
    "#         batch_masks[i, :num_points] = sample['target'].to(device)\n",
    "    \n",
    "#     offset, count = [], 0\n",
    "#     for item in max_points: # len of pc\n",
    "#         count += item.shape[0]\n",
    "#         offset.append(count)   \n",
    "#     return {\n",
    "#         'coord': batch_coordinates,\n",
    "#         'feat': batch_features,\n",
    "#         'offset': torch.IntTensor(batch_offsets),\n",
    "#         'prompt': batch_prompts,\n",
    "#         'target': batch_masks\n",
    "#     }\n",
    "def TrainValCollateFn(batch):\n",
    "    coord, feat, prompt, mask = list(zip(*batch))\n",
    "    offset, count = [], 0\n",
    "    for item in coord: # len of pc\n",
    "        count += item.shape[0]\n",
    "        offset.append(count)\n",
    "    \n",
    "    print(\"Coordinates type:\", type(coord[0]))  # Check the type of the first coordinate set\n",
    "    print(\"Features type:\", type(feat[0]))      # Check the type of the first features set\n",
    "    print(\"Mask type:\", type(mask[0]))          # Check the type of the first mask\n",
    "    print(\"Offset type:\", type(offset[0]))      # Check the type of the first offset\n",
    "    \n",
    "    data_dict = \\\n",
    "        {\n",
    "            'coord': torch.cat(coord).to(device),\n",
    "            'feat': torch.cat(feat).to(device),\n",
    "            'target': torch.cat(mask).to(device),\n",
    "            'prompt': list(prompt),\n",
    "            'offset': torch.IntTensor(offset).to(device),\n",
    "        }\n",
    "    return data_dict\n",
    "dataset = ArkitDataLoader(args.arkit_train_root,args.development_query_root,args.development_mask_root)\n",
    "#data_loader = DataLoader(dataset, batch_size= args.train_batch, collate_fn=TrainValCollateFn)\n",
    "data_loader = DataLoader(dataset, batch_size= 1, collate_fn=TrainValCollateFn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test input with Arkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cuda_memory_usage(device_id=0):\n",
    "    t = torch.cuda.get_device_properties(device_id).total_memory\n",
    "    r = torch.cuda.memory_reserved(device_id) \n",
    "    a = torch.cuda.memory_allocated(device_id)\n",
    "    f = r - a  # free inside reserved\n",
    "\n",
    "    print(f\"CUDA Device ID: {device_id}\")\n",
    "    print(f\"Total memory: {t / 1e9:.2f} GB\")\n",
    "    print(f\"Reserved memory: {r / 1e9:.2f} GB\")\n",
    "    print(f\"Allocated memory: {a / 1e9:.2f} GB\")\n",
    "    print(f\"Free (inside reserved): {f / 1e9:.2f} GB\")\n",
    "data_iterator = iter(data_loader)\n",
    "\n",
    "# 获取第一个元素\n",
    "first_batch = next(data_iterator)\n",
    "model(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "cls = nn.Sequential(\n",
    "    nn.Linear(32, 128), \n",
    "    nn.BatchNorm1d(128), \n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 512), \n",
    "    nn.BatchNorm1d(512), \n",
    "    nn.ReLU(inplace=True))\n",
    "cls = cls.to(device)\n",
    "\n",
    "\n",
    "#model.model.cls =cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_ftrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
